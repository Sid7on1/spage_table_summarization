{
  "agent_id": "coder2",
  "task_id": "task_2",
  "files": [
    {
      "filename": "utils.py",
      "purpose": "Utility functions for data processing and visualization",
      "priority": "low",
      "dependencies": [
        "numpy",
        "pandas",
        "matplotlib"
      ],
      "key_functions": [
        "load_data",
        "preprocess_data",
        "visualize_results"
      ],
      "estimated_lines": 200,
      "complexity": "low"
    }
  ],
  "project_info": {
    "project_name": "SPaGe_Table_Summarization",
    "project_type": "nlp",
    "description": "A framework for query-focused table summarization using structured planning and graph-based execution",
    "key_algorithms": [
      "Structured Planning",
      "Graph-based Execution",
      "SQL Query Generation"
    ],
    "main_libraries": [
      "transformers",
      "torch",
      "numpy",
      "pandas",
      "sqlalchemy"
    ]
  },
  "paper_content": "PDF: cs.CL_2507.22829v1_Beyond-Natural-Language-Plans-Structure-Aware-Pla.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nBeyond Natural Language Plans: Structure-Aware Planning for\nQuery-Focused Table Summarization\nWeijia Zhang1Songgaojun Deng1Evangelos Kanoulas1\n1IRLab, University of Amsterdam\nw.zhang2@uva.nl\nAbstract\nQuery-focused table summarization requires\ncomplex reasoning, often approached through\nstep-by-step natural language (NL) plans. How-\never, NL plans are inherently ambiguous and\nlack structure, limiting their conversion into\nexecutable programs like SQL and hindering\nscalability, especially for multi-table tasks. To\naddress this, we propose a paradigm shift to\nstructured representations. We introduce a new\nstructured plan, TaSoF, inspired by formalism\nin traditional multi-agent systems, and a frame-\nwork, SPaGe, that formalizes the reasoning pro-\ncess in three phases: 1) Structured Planning to\ngenerate TaSoF from a query 2) Graph-based\nExecution to convert plan steps into SQL and\nmodel dependencies via a directed cyclic graph\nfor parallel execution, and 3) Summary Gen-\neration to produce query-focused summaries.\nOur method explicitly captures complex depen-\ndencies and improves reliability. Experiments\non three public benchmarks show that SPaGe\nconsistently outperforms prior models in both\nsingle- and multi-table settings, demonstrating\nthe advantages of structured representations for\nrobust and scalable summarization.\n1 Introduction\nQuery-focused table summarization aims to gener-\nate a concise, textual summary from tabular data,\nspecifically tailored to a user\u2019s query. This task\nhas recently gained significant attention, emerging\nas a critical challenge in the field of table under-\nstanding (Zhao et al., 2023, 2024; Zhang et al.,\n2024a). A central hurdle in this domain is table rea-\nsoning, which involves accurately extracting and\naggregating query-relevant information from poten-\ntially complex input tables. Earlier studies predom-\ninantly focused on training end-to-end models that\nperformed reasoning implicitly (Liu et al., 2022b,a;\nJiang et al., 2022). While these approaches showed\npromising results, they often required extensive\nNatural LanguagePlan1.Identifythe namesofallstudents2.Assesspetownershipforeachstudent3.Returnthe namesofstudentswhoarepetownersOurStructuredPlanTable1:StudentTable2:Has_PetAgeNameID22Mary123Tom2Has_PetIDYes1No2What are the different names of students who have pets?InputQueryandTables\nID:3OPERATION:\u201cJoin\u201dSOURCE:[\u201cStep1\u201d,\u201cStep2\u201d]CONDITION:\u201cStep1.ID= Step2.ID\u201dOUTPUT:[\u201cStep2.ID\u201d,\u201cStep2.Name\u201d]Step3ID:1OPERATION:\u201cScan\u201dSOURCE:[\u201cHas_Pet\u201d]CONDITION:Has_Pet=\u201cYes\u201dOUTPUT:[\u201cID\u201d]Step1ID:2OPERATION:\u201cScan\u201dSOURCE:[\u201cStudent\u201d]CONDITION:nullOUTPUT:[\u201cID\u201d,\u201cName\u201d]Step2\nFigure 1: An example comparing a natural language\nplan with our proposed structured plan, TaSoF. The\nnatural language plan includes vague terms such as pet\nownership , which can be hard for LLMs to interpret. In\ncontrast, TaSoF follows standard class definitions and\nspecifies table-specific operations, making it easier for\nLLMs to understand.\ntraining data and lacked interpretability (Nguyen\net al., 2025).\nWith the rapid advancements in large language\nmodels (LLMs), recent research has shifted to-\nwards explicit, multi-step table reasoning. These\ncutting-edge methods typically leverage techniques\nlike query decomposition (Ye et al., 2023; Zhao\net al., 2024; Nguyen et al., 2025) or pre-defined\nworkflows (Wang et al., 2024; Zhang et al., 2024c;\nLi et al., 2024) to generate a step-by-step reasoning\nplan and then execute these steps to perform the\nnecessary table operations.\nHowever, a significant limitation persists: these\nreasoning plans are overwhelmingly generated as\nfree-form natural language (NL). This introduces\nsubstantial challenges because NL plans are in-\nherently unreliable and lack the explicit structure\ncrucial for robust execution. This linguistic ambi-\nguity makes the automated conversion of plan steps\ninto executable programs for table operations, such\nas SQL queries, a brittle and error-prone process;\neven minor variations in phrasing can lead to criti-\ncal execution failures (Zhao et al., 2024; NguyenarXiv:2507.22829v1  [cs.CL]  30 Jul 2025\n\n--- Page 2 ---\net al., 2025). Furthermore, the absence of a for-\nmal, machine-readable structure severely limits the\nscalability of these methods to complex multi-table\nscenarios and hinders opportunities for crucial exe-\ncution optimizations.\nTo address these limitations, we propose a\nparadigm shift from ambiguous NL plans to a for-\nmal, structured representation. Inspired by formal-\nism in traditional multi-agent systems (d\u2019Inverno\net al., 1997; Hilaire et al., 2000; Gruer et al., 2002),\nwe standardize the reasoning plan by defining it\nas a TableState and operation Flow (TaSoF). In\nthis framework, each planning step is defined as\naStep class with a schema. Specifically, the Step\nclass defines a specific table operation with its in-\nput and output states. An example of comparison\nbetween an NL plan and TaSoF is shown in Fig-\nure 1. From this example, we observe that natu-\nral language plans often include linguistic ambi-\nguities, such as the phrase \u201c pet ownership \u201d in the\nsecond step, which can be difficult for the LLM-\nbased execution module to interpret accurately. In\ncontrast, our structured plan adopts class defini-\ntions from software engineering, using explicit ex-\npressions like Has_Pet = \u201cYes\u201d , which are easier\nto understand and translate into executable pro-\ngrams. Built upon this, we then present SPaGe\n(Structured Planning andGraph-based execution),\na new framework designed to robustly handle both\nsingle- and multi-table summarization scenarios.\nOur framework operates in three distinct phases:\n1)Structured Planning : Given a complex user\nquery, an LLM-based planner generates a hierar-\nchical, multi-step structured plan in the TaSoF for-\nmat. This key-value structure explicitly models\nkey cross-step dependencies and is more concise\nand less ambiguous than its natural language coun-\nterpart. 2) Graph-based Execution : For each step\nwithin the structured plan, an executor leverages\nthe explicit key-value structure to convert the step\ninto a reliable executable SQL query. An external\ncode interpreter then executes this query, yielding\nthe intermediate output table. This structured rep-\nresentation enables us to model the entire plan as\na Directed Acyclic Graph (DAG), facilitating the\nefficient parallel execution of independent steps.\n3)Summary Generation : Once all plan steps have\nbeen successfully executed and intermediate results\nare obtained, a dedicated summary generator pro-\nduces a human-readable natural language summary\ntailored to the user query.\nWe thoroughly evaluate SPaGe against exist-ing methods on three public query-focused ta-\nble summarization datasets: FeTaQA, QTSumm,\nand QFMTS. Our experimental results consistently\ndemonstrate that our framework significantly out-\nperforms current baseline models, underscoring its\neffectiveness. Extensive analysis further reveals\nthe superiority of our proposed structured plan for-\nmat compared to natural language plans, leading\nto more robust and reliable execution. We also\nprovide qualitative analysis to illustrate how our\nstructured plan enables smoother and more accu-\nrate execution.\nTo the best of our knowledge, our method is the\nfirst to formalize ambiguous natural language plans\ninto a fully structured, machine-readable format\nspecifically for query-focused table summarization.\nOur key contributions are summarized as fol-\nlows:\n\u2022We define TaSoF, a novel structure that formal-\nizes reasoning plans as a flow of table states\nand operations, thereby improving execution ef-\nficiency and accuracy.\n\u2022We propose SPaGe, a new structure-aware\nframework that leverages this structured plan-\nning and graph-based execution to achieve su-\nperior performance on the query-focused table\nsummarization task.\n\u2022Extensive experiments and analysis on three\nbenchmarks demonstrate the effectiveness of our\napproach in complex multi-table reasoning and\nits ability to enable efficient parallel execution.\n2 Related Work\nThis section reviews two key areas of related work:\nprogram-augmented table reasoning and planning\nfor table understanding.\n2.1 Program-Augmented Table Reasoning\nTable reasoning necessitates models that can mimic\nhuman data analysis, encompassing operations like\narithmetic calculations (e.g., counting, aggregation)\nand the judicious filtering of irrelevant rows or\ncolumns. This capability is a fundamental com-\nponent for various table understanding tasks. Ear-\nlier approaches (Liu et al., 2022b; Jiang et al.,\n2022) primarily relied on end-to-end reasoning,\nfine-tuning black-box models to directly produce\ntask-specific outputs. A significant limitation of\nthese methods is their inherent lack of interpretabil-\nity, making it challenging to understand the intri-\ncate intermediate reasoning steps. More recent\n\n--- Page 3 ---\nstudies (Cheng et al., 2023; Ye et al., 2023; Zhang\net al., 2024b) have adopted program-augmented\nreasoning frameworks, often by employing tool-\naugmented LLMs (Schick et al., 2023). These ap-\nproaches generate executable, task-oriented code,\ntypically SQL or Python, to derive intermediate\nsub-tables or results. This paradigm significantly\nenhances interpretability by making the explicit\nreasoning steps traceable and verifiable. However,\nmost existing program-augmented methods primar-\nily target single-table scenarios, which restricts\ntheir direct applicability to complex real-world\ntasks that inherently demand reasoning across mul-\ntiple, interconnected tables. Our work explicitly\naddresses both single- and multi-table scenarios,\nand crucially, introduces a more efficient graph-\nbased execution paradigm to further optimize this\nprocess.\n2.2 Planning for Table Understanding\nThe remarkable advancements in large language\nmodels (LLMs) have spurred a surge of recent stud-\nies exploring planning mechanisms for various ta-\nble understanding tasks, including table summariza-\ntion (Zhao et al., 2023; Zhang et al., 2024a), table\nquestion answering (Pasupat and Liang, 2015; Nan\net al., 2022), and table fact verification (Chen et al.,\n2020). Existing LLM-based planning approaches\nin this domain can be broadly categorized into two\nmain paradigms: 1) Query Decomposition : This\nparadigm involves prompting LLMs to decompose\na complex, original query into a series of smaller,\nmore manageable sub-queries. These sub-queries\nare then addressed sequentially, with their interme-\ndiate results contributing to the final answer for the\noverall query (Ye et al., 2023; Zhao et al., 2024).\n2)Pre-defined Workflow : In this paradigm, LLMs\nare guided to iteratively select and execute opera-\ntions from a pre-defined set or to follow a sequence\nof human-engineered stages. This process contin-\nues until a specific stop condition is met, leading\nto the desired outcome (Wang et al., 2024; Zhang\net al., 2024c; Li et al., 2024; Mao et al., 2024).\nWhile these methods have demonstrated effective-\nness, particularly in single-table scenarios, they\npredominantly rely on natural language (NL) plans.\nA critical limitation of NL plans is their inherent\nlinguistic ambiguity, which can hinder robust and\nreliable subsequent execution processes. In stark\ncontrast, our work is the first to explicitly explore\nand formalize structure-based planning for both\nsingle- and multi-table scenarios, addressing thelimitations of NL-centric approaches.\nID:intOPERATION:strSOURCE:List[str]CONDITION:strOUTPUT:List[str]ClassStepID:1OPERATION:\u201cScan\u201dSOURCE:[\u201cHas_Pet\u201d]CONDITION:Has_Pet=\u201cYes\u201dOUTPUT:[\u201cID\u201d]Step1\nID:3OPERATION:\u201cJoin\u201dSOURCE:[\u201cStep1\u201d,\u201cStep2\u201d]CONDITION:\u201cStep1.ID= Step2.ID\u201dOUTPUT:[\u201cStep2.ID\u201d,\u201cStep2.Name\u201d]Step3ID:2OPERATION:\u201cScan\u201dSOURCE:[\u201cStudent\u201d]CONDITION:nullOUTPUT:[\u201cID\u201d,\u201cName\u201d]Step2\nFigure 2: Class definition of our structured plan TaSoF\nalongside an example plan instance.\n3 Problem Formulation.\nThe task of query-focused table summarization\ninvolves generating a query-specific summary s\ngiven a user query qand a collection of relational\ntables T={T1, T2, . . . , T n}. This process can\nbe expressed as a function mapping: (q,T)\u2192s,\nwhere sis tailored to capture the information rele-\nvant to q.\n4 Methodology\nIn this section, we first provide an overview of\nour proposed framework, followed by a detailed\nexplanation of each of its main components.\n4.1 Overview\nOur proposed framework follows a \u201cplan-then-\nexecute\u201d paradigm (Wang et al., 2023). It consists\nof three phases: Structure-based Planning ,Graph-\nbased Execution andSummary Generation , each\ncorresponding to a key component: a planner, an\nexecutor, and a summary generator. An overview\nof the framework is illustrated in Figure 3. Specifi-\ncally, during the planning phase, the planner gen-\nerates multiple atomic and structured steps based\non the given query qand tables T. In the execu-\ntion phase, the executor first identifies dependen-\ncies among these steps and constructs a directed\nacyclic graph (DAG) from the sequential steps to\ndetermine the optimal execution order. Next, the\nexecutor constructs and executes a corresponding\nSQL query for each step, leveraging the results of\nprevious steps to produce intermediate execution\ntables. Finally, the summary generator synthesizes\nthese execution tables into a coherent and concise\ntextual summary.\n\n--- Page 4 ---\nSummaryGenerator\nTablesQuerySummary\nExecutorExecutorPlanner\n(1)StructuredPlanning(2)Graph-basedExecution(3)SummaryGeneration\nExecutor\nPlan213Query\nFigure 3: The overview of our framework SPaGe. First, the planner generates a plan during structured planning.\nThen, the executor performs graph-based execution by constructing a directed acyclic graph based on step depen-\ndencies and executes each planning step in parallel according to the graph structure. Finally, the summary generator\nsynthesizes the last execution output into an informative summary tailored to the query.\nOperation Functionality\nScan Scan all table rows.\nAggregate Group and aggregate tuples.\nFilter Remove non-matching tuples.\nSort Sort stream by expression.\nTopSort Select top-K tuples.\nJoin Logically join two streams.\nExcept Compute set difference.\nIntersect Compute set intersection.\nUnion Compute set union.\nTable 1: Definition of the nine operations used in the\nstructured planning steps.\n4.2 Structured Planning\nStructured planning aims to formalize the plan-\nning process with well-defined, structured plan-\nning steps. Previous work on the plan-then-execute\nframework primarily relies on free-form natural lan-\nguage plans (Zhao et al., 2024; Nguyen et al., 2025).\nHowever, such plans inherently contain linguistic\nambiguities, making it challenging to automatically\nconvert plan steps into executable programs during\nexecution. This work instead focuses on generating\nwell-structured plans that align more effectively\nwith the execution phase. Inspired by formalism\nin traditional multi-agent systems (d\u2019Inverno et al.,\n1997; Hilaire et al., 2000; Gruer et al., 2002), we de-\nfine a plan as a sequence of well-defined, structured\nsteps.\nPlan Definition. Each step within the plan de-\nfines a table operation that transforms one or\nmore source tables into a result table. The\nclass definition for the step is shown in Figure 2.Specifically, we formulate a step as a standard\nclass with attributes from software engineering:\nSi={ID,OPERATION ,SOURCE ,CONDITION ,\nOUTPUT }. Here, IDdenotes the step identifier,\nandOPERATION specifies the table operation to\nbe performed. Inspired by the query plan language\n(QPL) (Eyal et al., 2023), we consider nine table\noperations that cover the most common scenarios.\nThe set of operations is shown in Table 1. The\nSOURCE field specifies the source tables involved\nin the operation. These may include the original in-\nput tables or the intermediate result tables returned\nby previous steps. The CONDITION field defines\nany constraints or filters applied during the oper-\nation, such as row or column selections, or keys\nused for table joins. Finally, OUTPUT denotes the\noutput columns of the resulting table.\nConsider the example plan consisting of three\nsteps shown in Figure 2, the step with the field ID\nequal to 1indicates the first step. It aims to extract\nthe column IDfrom the table Has_Pet with the\ncondition Has_Pet = \u201cYes\u201d .\nPlan Generation. To generate a structured plan,\nwe leverage the structured output capabilities of\nrecent LLMs, such as gpt-4o-mini . Given a\nuser query qand input tables T, we prompt the\nLLM to output a list of structured plans based on\nour predefined class schema. Since LLMs process\nonly textual inputs, we follow prior work (Liu et al.,\n2022b; Pal et al., 2023) and linearize each input\ntable into a text format before feeding it to the\nmodel. For a table with ncolumns and mrows, the\nlinearized version is:\n\n--- Page 5 ---\ntable name: name col:h1|. . .|hn\nrow 1: c1,1|. . .|c1,n. . . row m: cm,1|. . .|rm,n.\nPrevious approaches (Wang et al., 2024; Zhao\net al., 2024) typically linearize the entire table,\nwhich leads to a substantial increase in input length,\nespecially when tables have hundreds of rows.\nIn contrast, our experiments show that the table\nschema (i.e., column headers) plays the most crit-\nical role. Including only the top- krows ( k\u226am)\nretains performance while significantly reducing\ntoken usage.\n4.3 Graph-based Execution\nThe execution phase aims to obtain an intermedi-\nate execution table for each planning step si. We\nobserve that the execution order of steps can often\nbe parallelized, as different steps may depend on\ndistinct input tables or on separate preceding steps.\nConsider the example plan in Figure 2. We can ob-\nserve that the first step is independent of the second\nstep, as they rely on different source tables. There-\nfore, these two steps can be executed in parallel to\nimprove execution efficiency.\nGraph Construction. In general, each plan can\nbe formulated as a directed acyclic graph (DAG),\ndenoted as G= (V, E ), where V={s1, . . . , s n}\ndenotes the set of planning steps, and E={eij}\nrepresents the set of step dependencies. Specifi-\ncally, an edge eijexists if the output table from step\nsiis used as input to step sj. This graph-based for-\nmulation allows the framework to explicitly capture\ninter-step dependencies and enables more efficient\nexecution of the overall plan.\nProgram-Augmented Execution. Given a plan-\nning step, existing LLMs may struggle to generate\nwell-formatted tables. To address this limitation,\nwe enhance the LLM-based executor by incorpo-\nrating an external code interpreter. Specifically,\nthe executor leverages the code generation capa-\nbilities of the LLM to produce executable code.\nFollowing prior work (Cheng et al., 2023; Nguyen\net al., 2025), we prompt the LLM to generate a\nSQL query qsql\nifor each step si. To improve the\nrobustness and correctness of the generated SQL,\nwe include several demonstrations in the prompt\nto support in-context learning (Brown et al., 2020).\nThe code interpreter then executes the SQL query\nqsql\nito obtain the corresponding execution table.4.4 Summary Generation\nOnce the execution process is complete, we lever-\nage an LLM-based summary generator that takes\nthe original query qand the final table produced by\nthe last step snas input to generate a comprehen-\nsive summary. This summary is expected to inte-\ngrate all query-relevant information derived from\nthe input tables.\nProperty FeTaQA QTSumm QFMTS\nTable Source Wiki Wiki Database\nAvg. #Tables Per Example 1.0 1.0 1.8\nAvg. Summary Length 23.3 67.8 58.5\nTest Size 2,003 1,078 608\nTable 2: Basic statistics of the three benchmarks used\nin our experiments.\n5 Experimental Setup\nThis section presents the research questions and the\nexperimental setup used to evaluate our proposed\nframework. We describe the datasets, baseline mod-\nels, evaluation metrics, and implementation details.\n5.1 Research Questions\nOur experiments are designed to address the fol-\nlowing research questions (RQs):\n\u2022RQ1 : How does the proposed framework\nSPaGe perform compared to baseline mod-\nels?\n\u2022RQ2 : How does our TaSoF perform compared\nto natural language plan formats?\n\u2022RQ3 : How efficient is our graph-based execu-\ntion?\n5.2 Datasets\nTo evaluate the effectiveness of our approach, we\nutilize the three popular query-focused table sum-\nmarization datasets, including FeTaQA (Nan et al.,\n2022), QTSumm (Zhao et al., 2023), and QFMTS\ndataset (Zhang et al., 2024a). FeTaQA and QT-\nSumm are designed for single-table scenarios, both\nof which collect relational tables from Wikipedia.\nFeTaQA and QTSumm provide manually anno-\ntated query-summary pairs aligned with the corre-\nsponding tables. In contrast, the QFMTS dataset\nis designed for multi-table scenarios. It collects\nqueries and relational tables from the database and\nincludes annotated summaries tailored to the asso-\nciated queries. Key statistics of three datasets are\npresented in Table 2.\n\n--- Page 6 ---\n5.3 Baselines\nWe evaluate our approach by comparing it against\ntwo categories of baselines, including prompting-\nbased and program-augmented models.\nPrompt-based Models. These models aim to per-\nform table reasoning by only relying on the internal\nreasoning capabilities of LLMs themselves.\n\u2022CoT (Wei et al., 2022) uses Chain-of-\nThoughts (CoT) prompting to prompt LLMs\nto generate a summary given the query and\nrelational tables.\n\u2022ReFactor (Zhao et al., 2023) adapts heuristic\nmethods to extract query-relevant facts from\ngiven tables. Such facts are then concatenated\nwith a given query into context for LLMs to\ngenerate a summary.\n\u2022DirectSumm (Zhang et al., 2024a) tackles the\ntask in a single phase, which extracts query-\nrelevant facts from tables and synthesizes a\ncomprehensive query-focused summary based\non the facts jointly.\n\u2022Reason-then-Summ (Zhang et al., 2024a) ad-\ndresses the task in two sequential phases: ex-\ntracting query-relevant facts from tables and\nsynthesizing a comprehensive query-focused\nsummary based on the retrieved facts.\nProgram-Augmented Models. Unlike prompt-\nbased models, program-augmented models utilize\nthe code generation capabilities of LLMs to gener-\nate SQL queries and combine an external database\nengine to execute model-generated SQL queries,\nutilizing the execution results to augment the rea-\nsoning capabilities of LLMs.\n\u2022Binder (Cheng et al., 2023) converts the given\nNL query into either a Python program or a\nSQL query and executes the executable pro-\ngram to obtain the final answer. In our ex-\nperiments, we use the version of Binder that\ngenerates SQL queries for a fair comparison.\n\u2022Dater (Ye et al., 2023) leverages query de-\ncomposition to decompose a given query into\nsub-queries and convert each sub-query into\na SQL query, and aggregate the execution re-\nsults of the SQL query into a final summary.\n\u2022TaPERA (Zhao et al., 2024) generates natural\nlanguage plans by performing query decompo-\nsition and uses an executable Python programto obtain the intermediate answer for each\nplanning sub-query. Then it aggregates all\nintermediate query-answer pairs into a final\nsummary.\n5.4 Automated Evaluation\nFollowing Zhao et al. (2024), we evaluate the\nmodel\u2019s performance regarding the general quality\nof the generated summaries compared to the refer-\nence summaries, focusing on fluency and accuracy.\nWe consider the following metrics:\n\u2022BLEU (Papineni et al., 2002) is a precision-\noriented metric evaluating n-gram overlap be-\ntween generated and reference summaries.\nWe use SacreBLEU to compute BLEU scores.\n\u2022ROUGE (Lin and Hovy, 2003) measures\nrecall-oriented word overlap between gener-\nated and reference summaries. We report the\nF1 versions of ROUGE-L.\n\u2022METEOR (Banerjee and Lavie, 2005) mea-\nsures the unigram matching between the gen-\nerated and reference summaries.\n5.5 Implementation Details\nFor the planner, executor, and summary generator\nin our framework, we adopt gpt-4o-mini as the\nbackbone model due to its strong performance on\ntable understanding tasks (Nguyen et al., 2025). To\nenable effective in-context learning (Brown et al.,\n2020), we prepend the input prompt with 3-shot\ndemonstrations. We set the temperature, top-p,\nand maximum output tokens to 0.1,0.95, and 400,\nrespectively. For the baseline models, we follow\ntheir original papers to set their hyperparameters.\n6 Results and Analyses\n6.1 Main Results\nTo address RQ1 , we report the evaluation results\nin Table 3. The results show that SPaGe outper-\nforms all baseline models on both the FeTaQA and\nQFMTS datasets, with especially strong results on\nQFMTS.\nThe comparison between prompt-based and\nprogram-augmented methods shows that their\nperformance depends on the dataset. On QT-\nSumm, prompt-based models like Reason-then-\nSumm generally perform better. In contrast,\nprogram-augmented methods such as SPaGe and\n\n--- Page 7 ---\nMethodFeTaQA QTSumm QFMTS\nBLEU ROUGE-L METEOR BLEU ROUGE-L METEOR BLEU ROUGE-L METEOR\nPrompt-based\nCoT 28.2 51.0 56.9 19.3 39.0 47.2 31.5 54.3 58.1\nReFactor 26.2 53.6 57.2 19.9 39.5 48.8 \u2014 \u2014 \u2014\nDirectSumm 29.8 51.7 58.2 20.7 40.2 50.3 33.6 57.0 62.8\nReason-then-Summ 31.7 52.6 60.7 21.8 42.3 51.5 40.8 62.7 66.2\nProgram-Augmented\nBinder 25.5 47.9 51.1 18.2 40.0 39.0 42.5 65.3 70.7\nDater 29.8 54.0 59.4 16.6 35.2 35.5 \u2014 \u2014 \u2014\nTaPERA 29.5 53.4 58.2 14.6 33.0 33.2 \u2014 \u2014 \u2014\nSPaGe (Ours) 33.8 55.7 62.3 20.9 41.3 47.7 45.7 68.3 73.4\nTable 3: Automated evaluation of our approach and baseline models on the test sets of three benchmarks. FeTaQA\nand QTSumm are single-table datasets, while QFMTS is a multi-table dataset. The best and second-best results are\nshown in bold and underline , respectively.\nPlan Format BL R-L ME ESR\nQDMR 42.3 65.1 69.4 94.4\nQPL 43.4 66.2 71.5 95.1\nTaSoF (Ours) 45.7 68.3 73.4 98.2\nTable 4: Comparison of our TaSoF with natural lan-\nguage plan formats on the QFMTS test set. BL, R-L,\nME, and ESR denote BLEU, ROUGE-L, METEOR,\nand Execution Success Rate, respectively. ESR refers\nto the proportion of plan steps that can be successfully\nexecuted. The best are highlighted in bold .\nFeTAQA QTSumm QFMTS1.01.52.02.53.03.5Average Execution CyclesSEQ Ours\nFigure 4: Comparison of the average number of ex-\necution cycles between previous sequential execution\n(SEQ) and our graph-based execution (Ours).\nBinder achieve higher scores on QFMTS. This dif-\nference mainly comes from the structure of the\nsource tables. Tables in QTSumm, collected from\nWikipedia, often have multi-level and nested head-\ners. SQL queries struggle with these complex struc-\ntures, making it difficult for program-augmented\nmethods to extract the correct information. In con-\ntrast, QFMTS mainly contains standard relational\ntables, which are easier for program-augmented\nmethods to handle using SQL queries.6.2 Effect of Plan Formats\nTo address RQ2 , we compare TaSoF with natural\nlanguage plan formats, including QPL (Eyal et al.,\n2023) and QDMR (Wolfson et al., 2022). To en-\nsure a fair comparison, we adapt our planner to\ngenerate each plan format while keeping all other\ncomponents unchanged. The results are shown in\nTable 4. From the table, we see that TaSoF consis-\ntently outperforms the NL plan formats across all\nmetrics. For example, TaSoF achieves a METEOR\n(ME) score of 73.4, outperforming the next-best\nformat, QPL.\nSince program-augmented methods depend on\nthe LLM\u2019s ability to turn each plan step into ex-\necutable SQL, the accuracy of SQL generation is\ncritical. However, LLMs may produce invalid SQL,\nwhich can cause run-time errors during execution.\nThus, comparing the execution success rate across\nplan formats is important. As shown in Table 4,\nTaSoF achieves a significantly higher execution\nsuccess rate (ESR) of 98.2%, compared to 95.1%\nfor QPL. This demonstrates the advantage of TaSoF\nin both SQL execution accuracy and reliability.\n6.3 Efficiency of Graph-based Execution\nTo address RQ3 , we measure execution efficiency\nusing the number of execution cycles. In sequen-\ntial execution, step dependencies are ignored, and\nsteps are executed one after another. Therefore, the\nnumber of execution cycles is equal to the number\nof steps in the plan. In contrast, graph-based exe-\ncution accounts for step dependencies and enables\nparallel execution when possible. Here, the num-\nber of execution cycles corresponds to the length\nof the longest path in the DAG. For example, in\nthe plan shown in Figure 2, there are three steps.\nSteps 1 and 2 can be executed in parallel during the\n\n--- Page 8 ---\nChallenge Representative Example\nSemantic Mismatch\nbetween Query and\nTable SchemaQuery: What is the average duration of projects?\nTables:\nTable 1: Projects\nTable Schema: [ProjectID, ProjectName, StartDate, EndDate, Cost]\nNatural Language Plan :\n1.Find duration of each project.\n2.Calculate the average of these durations.\n3.Return the average duration for all projects.\nOur TaSoF {ID; OPERATION; SOURCE; CONDITION; OUTPUT }:\nStep1 ={1;Scan;[Projects] ;null;[ProjectID, StartDate, EndDate] }\nStep2 ={2;Aggregate ;[Step1] ;null;\n[ProjectID, (EndDate - StartDate) as Duration] }\nStep3 ={3;Aggregate ;[Step2] ;null;[AVG(Duration)] }\nComplex Step\nDependencies in\nMulti-Table\nScenariosQuery: List names and department locations of employees over 40 located in \u201cLondon\u201d.\nTables:\nTable 1: Employees\nTable Schema: [EmpID, EmpName, Age, DeptID]\nTable 2: Departments\nTable Schema: [DeptCode, DeptName, Location]\nNatural Language Plan :\n1.Get employees over 40 from the employees table.\n2.Find department locations in \u201cLondon\u201d from the departments table.\n3.Combine employee names and department locations .\nOur TaSoF {ID; OPERATION; SOURCE; CONDITION; OUTPUT }:\nStep1 ={1;Scan;[Employees] ;Age > 40 ;[EmpID, EmpName, DeptID] }\nStep2 ={2;Scan;[Departments] ;Location=\u201cLondon\u201d ];[DeptCode, Location }\nStep3 = {3;Join ;[Step1, Step2] ;Step1.DeptID = Step2.DeptCode ;\n[Step1.EmpName, Step2.Location] }\nTable 5: Qualitative comparison between the NL plan and our approach, TaSoF. We indicate two key challenges that\nlead to the failure of the NL plan. For simplicity, we flatten our class-based structured plan to improve readability.\nRed phrases highlight key errors in the NL plan, while Green phrases show how our plan addresses those errors.\nfirst cycle, while Step 3, which depends on both,\nruns in the second cycle. Thus, the plan requires 2\nexecution cycles.\nFigure 4 compares the average number of exe-\ncution cycles between sequential and graph-based\nexecution. The results show that graph-based exe-\ncution is more efficient, particularly on the QFMTS\ndataset. For example, it reduces the average num-\nber of execution cycles by about 28%. This im-\nprovement is due to QFMTS involving multi-table\ninputs, where steps are more likely to operate on\ndifferent input or intermediate tables. This allows\nfor greater parallelism in SQL generation and ex-\necution. Consequently, the performance gap be-\ntween the two execution methods is more pro-\nnounced for QFMTS than for FeTaQA and QT-\nSumm.\n6.4 Qualitative Analysis\nTo highlight the strengths of our structured plan,\nTaSoF, we conduct a qualitative analysis compar-\ning it to the NL plan, as shown in Table 5. The\nanalysis reveals two primary challenges that lead\nto the failure of the NL plan.\nThe first challenge is a semantic mismatch be-tween the query and the table schema. This occurs\ndue to unclear schema links between the query\nand the table. In the corresponding example, the\nquery uses the term duration , which is not present\nin the table schema. The NL plan also includes\nan invalid step, \u201c Find duration of each project. \u201d,\nwhich leads to execution failure. In contrast, TaSoF\nrestricts each step\u2019s output to use only existing\ncolumn names. It eliminates the ambiguity by cal-\nculating duration using the StartDate andEndDate\ncolumns based on their meaning, effectively resolv-\ning the mismatch.\nThe second challenge is complex step dependen-\ncies, which is especially problematic when multiple\ninput tables are involved. In the corresponding ex-\nample, the NL plan fails to connect the first two\nsteps properly. As a result, the final step wrongly\nperforms a union of their outputs, even though\nthe query requires their intersection. TaSoF ad-\ndresses this by using the SOURCE field to explicitly\nlink steps. For instance, Step3 citesStep1 and\nStep2 as its sources, ensuring the correct interpre-\ntation of the query.\n\n--- Page 9 ---\n7 Conclusion\nQuery-focused table summarization aims to cre-\nate concise, query-specific summaries from tabular\ndata. While recent advancements leverage large\nlanguage models (LLMs) for multi-step reasoning,\na key challenge persists: natural language (NL)\nreasoning plans are inherently ambiguous, leading\nto unreliable execution and hindering scalability.\nTo address this, we propose TaSoF, a novel, struc-\ntured plan format that formalizes a reasoning plan.\nBuilding on this, we introduce SPaGe, a frame-\nwork that employs structured planning and graph-\nbased execution for robust single- and multi-table\nsummarization. Our evaluations show that SPaGe\noutperforms existing methods, validating the effec-\ntiveness of our structured approach in achieving\nmore reliable and accurate table understanding.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Translation\nand/or Summarization , pages 65\u201372.\nTom B Brown, Benjamin Mann, and et al. 2020. Lan-\nguage Models are Few-Shot Learners. In Advances\nin Neural Information Processing Systems .\nWenhu Chen, Hongmin Wang, and et al. 2020. TabFact:\nA large-scale dataset for table-based fact verification.\nInICLR .\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2023. Binding language\nmodels in symbolic languages. In The Eleventh In-\nternational Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023 . Open-\nReview.net.\nMark d\u2019Inverno, Michael Fisher, Alessio Lomuscio,\nMichael Luck, Maarten de Rijke, Mark Ryan, and\nMichael J. Wooldridge. 1997. Formalisms for multi-\nagent systems. Knowledge Engineering Review ,\n12(3):315\u2013321.\nBen Eyal, Moran Mahabi, Ophir Haroche, Amir Bachar,\nand Michael Elhadad. 2023. Semantic decomposi-\ntion of question and SQL for text-to-SQL parsing.\nInFindings of the Association for Computational\nLinguistics: EMNLP 2023 , pages 13629\u201313645.\nPablo Gruer, Vincent Hilaire, Abder Koukam, and\nKrzysztof Cetnarowicz. 2002. A formal framework\nfor multi-agent systems analysis and design. Expert\nSystems With Applications , 23(4):349\u2013355.Vincent Hilaire, Abder Koukam, Pablo Gruer, and Jean-\nPierre M\u00fcller. 2000. Formal specification and pro-\ntotyping of multi-agent systems. In Engineering So-\ncieties in the Agent World, First International Work-\nshop, ESAW 2000, Berlin, Germany, August 21, 2000,\nRevised Papers , volume 1972 of Lecture Notes in\nComputer Science , pages 114\u2013127.\nZhengbao Jiang, Yi Mao, Pengcheng He, Graham Neu-\nbig, and Weizhu Chen. 2022. OmniTab: Pretraining\nwith natural and synthetic data for few-shot table-\nbased question answering. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 932\u2013942. Asso-\nciation for Computational Linguistics.\nQianlong Li, Chen Huang, Shuai Li, Yuanxin Xiang,\nDeng Xiong, and Wenqiang Lei. 2024. GraphOT-\nTER: Evolving LLM-based Graph Reasoning for\nComplex Table Question Answering. Preprint ,\narXiv:2412.01230.\nChin-Yew Lin and Eduard Hovy. 2003. Automatic eval-\nuation of summaries using N-gram co-occurrence\nstatistics. In NAACL , pages 150\u2013157.\nAo Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and\nDongmei Zhang. 2022a. PLOG: Table-to-logic\npretraining for logical table-to-text generation. In\nEMNLP .\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022b.\nTAPEX: Table pre-training via learning a neural SQL\nexecutor. In ICLR .\nQingyang Mao, Qi Liu, Zhi Li, Mingyue Cheng, Zheng\nZhang, and Rui Li. 2024. PoTable: Programming\nStandardly on Table-based Reasoning Like a Human\nAnalyst. Preprint , arXiv:2412.04272.\nLinyong Nan, Chiachun Hsieh, and et al. 2022. Fe-\nTaQA: Free-form table question answering. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:35\u201349.\nGiang Nguyen, Ivan Brugere, Shubham Sharma, San-\njay Kariyappa, Anh Totti Nguyen, and Freddy Lecue.\n2025. Interpretable LLM-based Table Question An-\nswering. Preprint , arXiv:2412.12386.\nVaishali Pal, Andrew Yates, Evangelos Kanoulas, and\nMaarten de Rijke. 2023. MultiTabQA: Generating\ntabular answers for multi-table question answering.\nInACL, pages 6322\u20136334.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In ACL, pages 311\u2013318.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nACL, pages 1470\u20131480.\n\n--- Page 10 ---\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. 2023.\nToolformer: Language models can teach themselves\nto use tools. In Advances in Neural Information Pro-\ncessing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023 .\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers) , pages 2609\u20132634.\nZilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin\nEisenschlos, Vincent Perot, Zifeng Wang, Lesly Mi-\nculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee,\nand Tomas Pfister. 2024. Chain-of-table: Evolving\ntables in the reasoning chain for table understanding.\nInThe Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS .\nTomer Wolfson, Daniel Deutch, and Jonathan Be-\nrant. 2022. Weakly supervised text-to-SQL parsing\nthrough question decomposition. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022 , pages 2528\u20132542.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language mod-\nels are versatile decomposers: Decomposing evi-\ndence and questions for table-based reasoning. In\nProceedings of the 46th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, SIGIR 2023, Taipei, Taiwan, July\n23-27, 2023 , pages 174\u2013184. ACM.\nWeijia Zhang, Vaishali Pal, Jia-Hong Huang, Evangelos\nKanoulas, and Maarten de Rijke. 2024a. QFMTS:\nGenerating query-focused summaries over multi-\ntable inputs. In Proceedings of the 27th European\nConference on Artificial Intelligence (ECAI) , pages\n3875\u20133882.\nYunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce\nCahoon, Shaleen Deep, and Jignesh M. Patel. 2024b.\nReAcTable: Enhancing ReAct for table question an-\nswering. Proc. VLDB Endow. , 17(8):1981\u20131994.\nZhehao Zhang, Yan Gao, and Jian-Guang Lou. 2024c.\nE5: Zero-shot hierarchical table analysis using aug-\nmented LLMs via explain, extract, execute, exhibit\nand extrapolate. In Proceedings of the 2024 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-guage Technologies (Volume 1: Long Papers) , pages\n1244\u20131258.\nYilun Zhao, Lyuhao Chen, Arman Cohan, and Chen\nZhao. 2024. TaPERA: Enhancing faithfulness and in-\nterpretability in long-form table QA by content plan-\nning and execution-based reasoning. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) ,\npages 12824\u201312840.\nYilun Zhao, Zhenting Qi, and et al. 2023. QTSumm:\nQuery-focused summarization over tabular data. In\nEMNLP , pages 1157\u20131172.",
  "project_dir": "artifacts/projects/SPaGe_Table_Summarization",
  "communication_dir": "artifacts/projects/SPaGe_Table_Summarization/.agent_comm",
  "assigned_at": "2025-07-31T21:46:36.928319",
  "status": "assigned"
}